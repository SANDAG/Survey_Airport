{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4833011",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef4bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "sys.path.insert(0, os.path.abspath(\"../data_model/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed990b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pydantic import ValidationError\n",
    "import data_model\n",
    "import enums as e\n",
    "from utils import extract_base_type, add_enum_label_columns, add_list_objects, add_synthetic_records, map_zones\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd4f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(data_model)\n",
    "importlib.reload(e)\n",
    "from data_model import Respondent, Employee, AirPassenger, Trip, DepartingPassengerResident, DepartingPassengerVisitor, ArrivingPassengerResident, ArrivingPassengerVisitor, DepartingAirPassenger, ArrivingAirPassenger, Resident, Visitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620468ee",
   "metadata": {},
   "source": [
    "### I/O Files:\n",
    "\n",
    "Raw Files provided by ETC:\n",
    "1. `od_20250314_sandag_airport_draftfinal.xlsx` - This includes the main intercept survey responses. \n",
    "2. `od_20250314_sandag_airport_pilotdata.xlsx` - This includes the pilot survey (also intercept) responses.\n",
    "3. `od_20253014_sandag_airport_sas_draftinal.xlsx` - This includes the self-administered survey (SAS) responses.\n",
    "4. `ATC_airport_travel_survey_SP_data_03212025.xlsx` - This includes the State Preference (SP) Survey responses. The respondents of the SP survey are a subset from the RP survey. \n",
    "\n",
    "Intermediate/Helper files:\n",
    "* `revised_names.csv` : This file acts as a mapping of variable names from the raw files to the data model's field names. Also has a 'delete' value to drop columns.\n",
    "\n",
    "Output Files:\n",
    "* `survey_data_clean.csv`: This is an intermediate file (an output of this script) which gets fed into the data model in the next script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2759821",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_dir = \"../data/external\"\n",
    "interim_dir = \"../data/interim\"\n",
    "processed_dir = \"../data/processed\"\n",
    "\n",
    "input_file1 = os.path.join(external_dir, \"etc/od_20250314_sandag_airport_draftfinal.xlsx\") #Main intercept survey responses\n",
    "input_file2 = os.path.join(external_dir, \"etc/od_20250314_sandag_airport_pilotdata.xlsx\") #Pilot responses\n",
    "input_file3 = os.path.join(external_dir, \"etc/od_20253014_sandag_airport_sas_draftinal.xlsx\") #Self administered survey responses\n",
    "input_file4 = os.path.join(external_dir, \"etc/ATC_airport_travel_survey_SP_data_03212025.xlsx\") #Stated Preference Survey Responses\n",
    "\n",
    "variable_map_file = os.path.join(processed_dir, \"revised_names.csv\")\n",
    "clean_survey_file = os.path.join(interim_dir, \"survey_data_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f389d5",
   "metadata": {},
   "source": [
    "### Clean Data , Rename fields\n",
    "\n",
    "This section does the following:\n",
    "1. Reads all the four files (including complete and incomplete sheets)\n",
    "2. Renames the columns using the `revised_names.csv` file, and drops unecessary columns\n",
    "3. Assigns a few new useful columns : to identify the record type and source, assign placeholder weights to records.\n",
    "4. Merges all datasets to a single dataset (by concating the RP datasets and joining the SP dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2cf10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df_complete1 = pd.read_excel(input_file1, sheet_name = 0)\n",
    "in_df_incomplete1 = pd.read_excel(input_file1, sheet_name = 1)\n",
    "\n",
    "in_df_complete2 = pd.read_excel(input_file2, sheet_name = 0)\n",
    "in_df_incomplete2 = pd.read_excel(input_file2, sheet_name = 1)\n",
    "\n",
    "in_df_complete3 = pd.read_excel(input_file3, sheet_name = 0)\n",
    "in_df_incomplete3 = pd.read_excel(input_file3, sheet_name = 1)\n",
    "\n",
    "in_df_sp = pd.read_excel(input_file4, sheet_name = 1)\n",
    "\n",
    "in_df_complete2['is_self_administered'], in_df_incomplete2['is_self_administered'] = False, False\n",
    "in_df_complete1['is_self_administered'], in_df_incomplete1['is_self_administered'] = False, False\n",
    "in_df_complete3['is_self_administered'], in_df_incomplete3['is_self_administered'] = True, True\n",
    "\n",
    "\n",
    "header_df = pd.read_csv(variable_map_file)[['ETC_name','WSP_name']]\n",
    "header_dict = pd.Series(header_df.WSP_name.values,index=header_df.ETC_name).to_dict()\n",
    "\n",
    "in_df_complete1 = in_df_complete1.rename(columns=header_dict).copy().drop(columns=[\"delete\"])\n",
    "in_df_complete2 = in_df_complete2.rename(columns=header_dict).copy().drop(columns=[\"delete\"])\n",
    "in_df_complete3 = in_df_complete3.rename(columns=header_dict).copy().drop(columns=[\"delete\"])\n",
    "\n",
    "\n",
    "in_df_incomplete1 = in_df_incomplete1.rename(columns=header_dict).copy().drop(columns=[\"delete\"])\n",
    "in_df_incomplete2 = in_df_incomplete2.rename(columns=header_dict).copy().drop(columns=[\"delete\"])\n",
    "in_df_incomplete3 = in_df_incomplete3.rename(columns=header_dict).copy().drop(columns=[\"delete\"])\n",
    "in_df_sp = in_df_sp.rename(columns=header_dict).copy().drop(columns=[\"delete\"])\n",
    "\n",
    "\n",
    "in_df_complete = pd.concat([in_df_complete1, in_df_complete2, in_df_complete3], ignore_index = True)\n",
    "in_df_incomplete = pd.concat([in_df_incomplete1, in_df_incomplete2, in_df_incomplete3], ignore_index = True)\n",
    "\n",
    "in_df_complete['is_completed'] = 1\n",
    "in_df_incomplete['is_completed'] = 0\n",
    "\n",
    "#Populating all weights columns\n",
    "\n",
    "in_df_complete['weight'] = 1\n",
    "in_df_complete['weight_departing_and_arriving'] = 1\n",
    "in_df_complete['weight_departing_only'] = 1\n",
    "in_df_complete['weight_non_sas_departing_only'] = 1\n",
    "in_df_complete['weight_departing_only_with_time_of_day'] = 1\n",
    "\n",
    "in_df_incomplete['weight'] = 0\n",
    "in_df_incomplete['weight_departing_and_arriving'] = 0\n",
    "in_df_incomplete['weight_departing_only'] = 0\n",
    "in_df_incomplete['weight_non_sas_departing_only'] = 0\n",
    "in_df_incomplete['weight_departing_only_with_time_of_day'] = 0\n",
    "\n",
    "\n",
    "#Concat incomplete and complete dataframes\n",
    "clean_df = pd.concat([in_df_complete, in_df_incomplete], ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['is_self_administered'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab00dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Complete Records: \", in_df_complete.shape)\n",
    "print(\"Incomplete Records: \", in_df_incomplete.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dec692",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c5017",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_df['respondentid'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e9c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the duplicate respondentids\n",
    "clean_df.drop_duplicates('respondentid', keep = 'first', inplace = True)\n",
    "clean_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd74b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates from SP (keep only valid records)\n",
    "in_df_sp = in_df_sp[in_df_sp['sp_is_valid'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge SP\n",
    "clean_df = clean_df.merge(in_df_sp, on=\"respondentid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1686f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c75691",
   "metadata": {},
   "source": [
    "#### Add Zones Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5414e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PMSA\n",
    "pmsa_zones_shapefile = \"../data/external/geometry/pmsa_geoms/pmsa_geoms.shp\"\n",
    "clean_df['origin_pmsa'] = map_zones(clean_df, 'origin_latitude', 'origin_longitude', pmsa_zones_shapefile, 'pseudomsa', 99)\n",
    "clean_df['destination_pmsa'] = map_zones(clean_df, 'destination_latitude', 'destination_longitude', pmsa_zones_shapefile, 'pseudomsa', 99)\n",
    "clean_df['origin_pmsa'].value_counts(), clean_df['destination_pmsa'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb5c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Municipal Zones\n",
    "municipal_zones_shapefile = \"../data/external/geometry/Municipal_Boundaries/Municipal_Boundaries.shp\"\n",
    "clean_df['origin_municipal_zone'] = map_zones(clean_df, 'origin_latitude', 'origin_longitude', municipal_zones_shapefile, 'name', 'EXTERNAL')\n",
    "clean_df['destination_municipal_zone'] = map_zones(clean_df, 'destination_latitude', 'destination_longitude', municipal_zones_shapefile, 'name', 'EXTERNAL')\n",
    "clean_df['origin_municipal_zone'].value_counts(), clean_df['destination_municipal_zone'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d47497",
   "metadata": {},
   "source": [
    "### Making all modes consistent\n",
    "\n",
    "This section aims at making the Enums used for all modes consistent, so that they follow the same mapping from `Enums.TravelMode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec161a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['egress_mode_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['other_airport_accessmode_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70042bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_airport_accessmode_label_map = {\n",
    "    'Walk': 'Walk',\n",
    "    'Wheelchair or other mobility device': 'Wheelchair or other mobility device',\n",
    "    'ELECTRIC BIKESHARE': 'Bicycle: electric bikeshare',\n",
    "    'NON ELECTRIC BIKESHARE': 'Bicycle: non-electric bikeshare',\n",
    "    'E SCOOTER SHARE': 'E-scooter: shared',\n",
    "    'PERSONAL ELECTRIC BICYCLE': 'Bicycle: personal electric bicycle',\n",
    "    'PERSONAL NON ELECTRIC BICYCLE': 'Bicycle: personal non-electric bicycle',\n",
    "    'PERSONAL E SCOOTER': 'E-scooter: personal',\n",
    "    'Taxi': 'Taxi',\n",
    "    'UBER LYFT': 'Uber/Lyft',\n",
    "    'CAR SERVICE BLACK CAR LIMO EXECUTIVE CAR': 'Car service/black car/limo/executive car',\n",
    "    'DROPPED OFF BY CAR BY FRIEND FAMILY': 'Dropped off by car by family/friend',\n",
    "    'Drove alone and parked': 'Drove alone and parked',\n",
    "    'Drove with others and parked': 'Drove with others and parked',\n",
    "    'RODE WITH OTHER TRAVELER AND PARKED': 'Rode with other traveler(s) and parked',\n",
    "    'Other public transit': 'Other public transit',\n",
    "    'Chartered tour bus': 'Chartered tour bus',\n",
    "    'Employee shuttle': 'Employee shuttle',\n",
    "    'RENTAL CAR AND DROPPED IT OFF AT RENTAL AGENCY': 'Rental car: Dropped off at rental agency',\n",
    "    'RENTAL CAR AND PARKED IT': 'Rental car: parked rental car',\n",
    "    'Hotel shuttle van': 'Hotel shuttle van',\n",
    "    'OTHER SHARED RIDE VAN SERVICE': 'Other shared van (please specify)',\n",
    "    'Other': 'Other',\n",
    "    'Refused/No Answer': 'Refused/No Answer'\n",
    "}\n",
    "clean_df['other_airport_accessmode_label'] = clean_df['other_airport_accessmode_label'].map(other_airport_accessmode_label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb3b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['other_airport_accessmode_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df29c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_mode_dict = {\n",
    "    'Walk': 1,\n",
    "    'Wheelchair or other mobility device': 2,\n",
    "    'Bicycle: electric bikeshare': 3,\n",
    "    'Bicycle: non-electric bikeshare': 4,\n",
    "    'E-scooter: shared': 5,\n",
    "    'Bicycle: personal electric bicycle': 6,\n",
    "    'Bicycle: personal non-electric bicycle': 7,\n",
    "    'E-scooter: personal': 8,\n",
    "    'Taxi': 9,\n",
    "    'Uber/Lyft': 10,\n",
    "    'Car service/black car/limo/executive car': 11,\n",
    "    'Dropped off by car by family/friend': 12,\n",
    "    'Drove alone and parked': 13,\n",
    "    'Drove with others and parked': 14,\n",
    "    'MTS Route 992': 15,\n",
    "    'Airport flyer shuttle': 16,\n",
    "    'Chartered tour bus': 17,\n",
    "    'Employee shuttle': 18,\n",
    "    'Rental car: Dropped off at rental agency': 19,\n",
    "    'Rental car: parked rental car': 20,\n",
    "    'Hotel shuttle van': 21,\n",
    "    'Other shared van (please specify)': 22,\n",
    "    'Picked up by car by family/friend': 23,\n",
    "    'Get in a parked vehicle and drive alone': 24,\n",
    "    'Get in a parked vehicle and drive with others': 25,\n",
    "    'Get in a parked vehicle and ride with other traveler(s)': 26,\n",
    "    'Rental car: Picked up at rental agency': 27,\n",
    "    'Rental car: get in a parked rental car': 28,\n",
    "    'Rode with other traveler(s) and parked': 29,\n",
    "    'Other public transit': 30,\n",
    "    'Public Transit': 30,\n",
    "    'Other': 98,\n",
    "    'Refused/No Answer': 99,\n",
    "    'None of the above': 98\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be478ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modes to fix\n",
    "mode_code_columns = ['main_transit_mode', 'main_mode', 'access_mode', 'egress_mode', 'reverse_mode', 'reverse_mode_predicted', 'other_airport_accessmode', 'reverse_commute_mode']\n",
    "mode_label_columns = ['main_transit_mode_label', 'main_mode_label', 'access_mode_label', 'egress_mode_label', 'reverse_mode_label', 'reverse_mode_predicted_label', 'other_airport_accessmode_label', 'reverse_commute_mode_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf0ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remapping codes using label strings\n",
    "travel_mode_dict = {k.lower(): v for k, v in travel_mode_dict.items()}\n",
    "for mode_code_col, mode_label_col in zip(mode_code_columns, mode_label_columns):\n",
    "    # Apply the mapping for each pair of columns\n",
    "    clean_df[mode_code_col] = clean_df[mode_label_col].str.lower().map(travel_mode_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d33510",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['other_airport_accessmode_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2195ca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['other_airport_accessmode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b5bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['main_transit_mode'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebfbb23",
   "metadata": {},
   "source": [
    "### Pre-processing of some fields\n",
    "\n",
    "This section aims at converting a few more inconsistent values to be consistent with the enums defined as the part of the data model.\n",
    "For instance, it converts String codes to Integers, to keep the code-label format standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['date_completed'] = pd.to_datetime(clean_df['date_completed'])\n",
    "clean_df['is_pilot'] = np.where(clean_df['date_completed'].dt.date<=datetime.date(2024, 10, 3), 1, 0)\n",
    "clean_df['record_type_synthetic'] = 0\n",
    "clean_df.replace('-oth-', 98, inplace=True)\n",
    "clean_df.replace('-', None, inplace = True )\n",
    "clean_df['is_income_below_poverty'] = np.where(clean_df['is_income_below_poverty'] == 0, 2, clean_df['is_income_below_poverty'])\n",
    "clean_df['household_income'] = np.where(clean_df['household_income']=='13B', 17, clean_df['household_income'] )\n",
    "\n",
    "clean_df['stay_informed'] = np.where(clean_df['stay_informed'] == 0, 2, clean_df['stay_informed'])\n",
    "#Maps\n",
    "interview_location_map = {'Term1' : 1, 'Term2': 2, 'MTS_1_992': 3, 'SDA_1_FLYER': 4, 'ConracShuttle': 5, 'ParkingShuttle': 6, 'EmplParking': 7, '-oth-':98} \n",
    "inbound_outbound_map = {'IN':1, 'OUT':2}\n",
    "\n",
    "#route_fields:\n",
    "route_fields = ['to_airport_transit_route_1', 'to_airport_transit_route_2', 'to_airport_transit_route_3', 'to_airport_transit_route_4',\n",
    "                'from_airport_transit_route_1', 'from_airport_transit_route_2', 'from_airport_transit_route_3', 'from_airport_transit_route_4']\n",
    "\n",
    "#Replacement\n",
    "clean_df['interview_location'] = clean_df['interview_location'].map(interview_location_map)\n",
    "clean_df['inbound_or_outbound'] = clean_df['inbound_or_outbound'].map(inbound_outbound_map)\n",
    "clean_df['main_mode'] = np.where(clean_df['main_transit_mode'].isin([15,16]), clean_df['main_transit_mode'], clean_df['main_mode'])\n",
    "\n",
    "clean_df[route_fields] = clean_df[route_fields].replace(98, 'OTHER')\n",
    "clean_df['nights_visited'] = clean_df['nights_visited'] - 1\n",
    "\n",
    "clean_df['same_commute_mode'] = np.where(clean_df['same_commute_mode'] == 0, 2, clean_df['same_commute_mode'])\n",
    "clean_df['resident_visitor_followup'] = np.where(clean_df['resident_visitor_followup'] == 0, 2, clean_df['resident_visitor_followup'])\n",
    "\n",
    "#activity_type\n",
    "clean_df['origin_activity_type'] = np.where(clean_df['inbound_or_outbound'] == e.InboundOutbound.OUTBOUND_FROM_AIRPORT, e.ActivityType.SAN_DIEGO_AIRPORT, clean_df['origin_activity_type'])\n",
    "clean_df['destination_activity_type'] = np.where(clean_df['inbound_or_outbound'] == e.InboundOutbound.INBOUND_TO_AIRPORT, e.ActivityType.SAN_DIEGO_AIRPORT, clean_df['destination_activity_type'])\n",
    "\n",
    "#For incomplete records:\n",
    "clean_df['marketsegment'] = clean_df['marketsegment'].fillna(99)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f315a",
   "metadata": {},
   "source": [
    "### Fix main_mode to not take EMPLOYEE_SHUTTLE\n",
    "\n",
    "EMPLOYEE_SHUTTLE has been disqualified as being a main_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815c14aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['main_mode'] = np.where(\n",
    "    clean_df['main_mode'] == e.TravelMode.EMPLOYEE_SHUTTLE,\n",
    "    np.where(\n",
    "        clean_df['marketsegment'] == e.Type.PASSENGER,\n",
    "        e.TravelMode.OTHER,\n",
    "        np.where(\n",
    "            clean_df['marketsegment'] == e.Type.EMPLOYEE,\n",
    "            clean_df['reverse_commute_mode'],\n",
    "            clean_df['main_mode']  # fallback if neither condition is met\n",
    "        )\n",
    "    ),\n",
    "    clean_df['main_mode']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ba9f2",
   "metadata": {},
   "source": [
    "### Re-assign some main_mode_others to main_mode categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1c8d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[['marketsegment_label', 'main_mode_other']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d2a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modes which are invalid, should make main_mode blank, and hence throw a critical validation error\n",
    "# Some of the modes can stay as they are (i.e., OTHER) - Like, Refugee Shuttle, Hospital Shuttle, Medical Shuttle. \n",
    "# Others can be classified - for example \n",
    "# Mapping for reclassification\n",
    "mode_mapping = {\n",
    "    \"Hospital shuttle\": e.TravelMode.OTHER,\n",
    "    \"Medical shuttle\": e.TravelMode.OTHER,\n",
    "    \"Refugee shuttle\": e.TravelMode.OTHER,\n",
    "    \"Motorcycle\": e.TravelMode.OTHER,\n",
    "    \"Bus\": e.TravelMode.OTHER_PUBLIC_TRANSIT,\n",
    "    \"Connecting flights\": None,\n",
    "    \"Airplane\": None,\n",
    "    \"Flew in\": None,\n",
    "    \"Medical\": e.TravelMode.OTHER,\n",
    "    \"Personal car\": e.TravelMode.DROVE_ALONE_AND_PARKED,\n",
    "    \"Paratransit\": e.TravelMode.OTHER,\n",
    "    \"Shelter\": None,\n",
    "    \"Stayed with family near airport and they drove me\": e.TravelMode.DROPPED_OFF_BY_FAMILY_FRIEND,\n",
    "    \"Team bus\": e.TravelMode.CHARTERED_TOUR_BUS,\n",
    "    \"Personal shuttle\": e.TravelMode.OTHER_SHARED_VAN,\n",
    "    \"Turo\": e.TravelMode.RENTAL_CAR_PICKED_UP,\n",
    "    \"Work\": None,\n",
    "    \"Flight\": None,\n",
    "    \"Flew\": None,\n",
    "    \"Mts blue line\": e.TravelMode.OTHER_PUBLIC_TRANSIT,\n",
    "    \"Route 10 and then Employee Shuttle\": e.TravelMode.OTHER_PUBLIC_TRANSIT,\n",
    "    \"Telecommute Day but on a working day I use the hours below\": None,\n",
    "    \"Work from home today\": None\n",
    "}\n",
    "\n",
    "# Create a mapped column without modifying main_mode yet\n",
    "# Create a mapped column\n",
    "mapped_modes = clean_df[\"main_mode_other\"].map(mode_mapping)\n",
    "\n",
    "# Update main_mode where main_mode_other exists in mode_mapping (including None values)\n",
    "clean_df.loc[clean_df[\"main_mode_other\"].isin(mode_mapping.keys()), \"main_mode\"] = mapped_modes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abdda2d",
   "metadata": {},
   "source": [
    "### Create Grouped Modes\n",
    "This section creates grouped modes for better readability and analysis. Particularly, it makes modes direction-agnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773afd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_mode_to_grouped = {\n",
    "    e.TravelMode.WALK: e.TravelModeGrouped.WALK,\n",
    "    e.TravelMode.WHEELCHAIR_OR_MOBILITY_DEVICE: e.TravelModeGrouped.WHEELCHAIR_OR_OTHER_MOBILITY_DEVICE,\n",
    "    e.TravelMode.BICYCLE_ELECTRIC_BIKESHARE: e.TravelModeGrouped.MICROMOBILITY_SHARED,\n",
    "    e.TravelMode.BICYCLE_NON_ELECTRIC_BIKESHARE: e.TravelModeGrouped.MICROMOBILITY_SHARED,\n",
    "    e.TravelMode.BICYCLE_PERSONAL_ELECTRIC: e.TravelModeGrouped.MICROMOBILITY_PERSONAL,\n",
    "    e.TravelMode.BICYCLE_PERSONAL_NON_ELECTRIC: e.TravelModeGrouped.MICROMOBILITY_PERSONAL,\n",
    "    e.TravelMode.E_SCOOTER_SHARED: e.TravelModeGrouped.MICROMOBILITY_SHARED,\n",
    "    e.TravelMode.E_SCOOTER_PERSONAL: e.TravelModeGrouped.MICROMOBILITY_PERSONAL,\n",
    "    e.TravelMode.TAXI: e.TravelModeGrouped.RIDEHAIL_TAXI,\n",
    "    e.TravelMode.UBER_LYFT: e.TravelModeGrouped.RIDEHAIL_TAXI,\n",
    "    e.TravelMode.CAR_SERVICE_BLACK_LIMO: e.TravelModeGrouped.RIDEHAIL_TAXI,\n",
    "    e.TravelMode.MTS_ROUTE_992: e.TravelModeGrouped.BUS_992,\n",
    "    e.TravelMode.AIRPORT_FLYER_SHUTTLE: e.TravelModeGrouped.AIRPORT_FLYER_SHUTTLE,\n",
    "    e.TravelMode.OTHER_PUBLIC_TRANSIT: e.TravelModeGrouped.PUBLIC_TRANSPORTATION,\n",
    "    e.TravelMode.DROPPED_OFF_BY_FAMILY_FRIEND: e.TravelModeGrouped.PERSONAL_CAR_DROPPED_OFF_PICKED_UP,\n",
    "    e.TravelMode.PICKED_UP_BY_FAMILY_FRIEND: e.TravelModeGrouped.PERSONAL_CAR_DROPPED_OFF_PICKED_UP,\n",
    "    e.TravelMode.DROVE_ALONE_AND_PARKED: e.TravelModeGrouped.PERSONAL_CAR_PARKED,\n",
    "    e.TravelMode.DROVE_WITH_OTHERS_AND_PARKED: e.TravelModeGrouped.PERSONAL_CAR_PARKED,\n",
    "    e.TravelMode.RODE_WITH_OTHER_TRAVELERS_AND_PARKED: e.TravelModeGrouped.PERSONAL_CAR_PARKED,\n",
    "    e.TravelMode.GET_IN_PARKED_VEHICLE_AND_DRIVE_ALONE: e.TravelModeGrouped.PERSONAL_CAR_PARKED,\n",
    "    e.TravelMode.GET_IN_PARKED_VEHICLE_AND_DRIVE_WITH_OTHERS: e.TravelModeGrouped.PERSONAL_CAR_PARKED,\n",
    "    e.TravelMode.GET_IN_PARKED_VEHICLE_AND_RIDE_WITH_OTHER_TRAVELERS: e.TravelModeGrouped.PERSONAL_CAR_PARKED,\n",
    "    e.TravelMode.RENTAL_CAR_DROPPED_OFF: e.TravelModeGrouped.RENTAL_CAR,\n",
    "    e.TravelMode.RENTAL_CAR_PARKED: e.TravelModeGrouped.RENTAL_CAR,\n",
    "    e.TravelMode.RENTAL_CAR_PICKED_UP: e.TravelModeGrouped.RENTAL_CAR,\n",
    "    e.TravelMode.RENTAL_CAR_GET_IN_PARKED: e.TravelModeGrouped.RENTAL_CAR,\n",
    "    e.TravelMode.HOTEL_SHUTTLE_VAN: e.TravelModeGrouped.SHARED_SHUTTLE_VAN,\n",
    "    e.TravelMode.EMPLOYEE_SHUTTLE: e.TravelModeGrouped.SHARED_SHUTTLE_VAN,\n",
    "    e.TravelMode.OTHER_SHARED_VAN: e.TravelModeGrouped.SHARED_SHUTTLE_VAN,\n",
    "    e.TravelMode.CHARTERED_TOUR_BUS: e.TravelModeGrouped.OTHER,\n",
    "    e.TravelMode.OTHER: e.TravelModeGrouped.OTHER,\n",
    "    e.TravelMode.REFUSED_NO_ANSWER: e.TravelModeGrouped.REFUSED_NO_ANSWER,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20809722",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_columns_to_remap = ['main_mode', 'access_mode', 'egress_mode', 'reverse_mode', 'reverse_mode_predicted', 'other_airport_accessmode', 'reverse_commute_mode']\n",
    "for col in mode_columns_to_remap:\n",
    "    clean_df[f'{col}_grouped'] = clean_df[col].map(travel_mode_to_grouped)\n",
    "    print(f\"Remapping Done for {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe693541",
   "metadata": {},
   "source": [
    "### Consolidating multiple columns into one string column:\n",
    "A few columns can be merged into one single list, instead of being represented as a long list of columns. This is done to deliver a cleaner dataset with fewer columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab605f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "general_modes_used_visitor_mode_columns = [col for col in clean_df.columns if col.startswith(\"general_modes_used_visitor\")]\n",
    "alt_commute_mode_columns = [col for col in clean_df.columns if col.startswith(\"alt_commute_mode\")]\n",
    "sdia_accessmode_split_columns = [col for col in clean_df.columns if col.startswith(\"sdia_accessmode_split_\")]\n",
    "race_columns = [col for col in clean_df.columns if col.startswith(\"race_\")]\n",
    "reasons_no_transit_columns = [col for col in clean_df.columns if col.startswith(\"reasons_no_transit_\")]\n",
    "party_composition_columns = [col for col in clean_df.columns if col.startswith(\"party_includes_\")]\n",
    "\n",
    "\n",
    "# Create a new column with a comma-separated list of active modes\n",
    "clean_df[\"general_modes_used_visitor_list\"] = clean_df[general_modes_used_visitor_mode_columns].apply(lambda row: \n",
    "    \", \".join([col.replace(\"general_modes_used_visitor_\", \"\").replace(\"_\", \" \") for col in general_modes_used_visitor_mode_columns if row[col]=='Yes']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "clean_df[\"alt_commute_mode_list\"] = clean_df[alt_commute_mode_columns].apply(lambda row: \n",
    "    \", \".join([col.replace(\"alt_commute_mode_\", \"\").replace(\"_\", \" \") for col in alt_commute_mode_columns if row[col]=='Yes']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "clean_df[\"sdia_accessmode_split_list\"] = clean_df[sdia_accessmode_split_columns].apply(lambda row:\n",
    "    \", \".join([col.replace(\"sdia_accessmode_split_\", \"\").replace(\"_\", \" \") for col in sdia_accessmode_split_columns if row[col]=='Yes']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "clean_df[\"race_list\"] = clean_df[race_columns].apply(lambda row:\n",
    "    \", \".join([col.replace(\"race_\", \"\").replace(\"_\", \" \") for col in race_columns if row[col]=='Yes']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "clean_df[\"reasons_no_transit_list\"] = clean_df[reasons_no_transit_columns].apply(lambda row:\n",
    "    \", \".join([col.replace(\"reasons_no_transit_\", \"\").replace(\"_\", \" \") for col in reasons_no_transit_columns if row[col]=='Yes']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "clean_df['party_composition_list'] = clean_df[party_composition_columns].apply(lambda row:\n",
    "    \", \".join([col.replace(\"party_includes_\", \"\").replace(\"_\", \" \") for col in party_composition_columns if row[col]=='Yes']), \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6cfb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_transit_columns = [\n",
    "    \"from_airport_transit_route_1\", \"from_airport_transit_route_1_other\",\n",
    "    \"from_airport_transit_route_2\", \"from_airport_transit_route_2_other\",\n",
    "    \"from_airport_transit_route_3\", \"from_airport_transit_route_3_other\",\n",
    "    \"from_airport_transit_route_4\", \"from_airport_transit_route_4_other\",\n",
    "    \"to_airport_transit_route_1\", \"to_airport_transit_route_1_other\",\n",
    "    \"to_airport_transit_route_2\", \"to_airport_transit_route_2_other\",\n",
    "    \"to_airport_transit_route_3\", \"to_airport_transit_route_3_other\",\n",
    "    \"to_airport_transit_route_4\", \"to_airport_transit_route_4_other\"\n",
    "]\n",
    "\n",
    "# Ensure only valid columns (those that exist in the DataFrame)\n",
    "valid_transit_columns = [col for col in ordered_transit_columns if col in clean_df.columns]\n",
    "\n",
    "# Concatenate only non-null values while maintaining the correct order\n",
    "clean_df[\"transit_routes_list\"] = clean_df[valid_transit_columns].apply(\n",
    "    lambda row: \", \".join(row.dropna().astype(str)), axis=1\n",
    ")\n",
    "\n",
    "# Compute the number of transfers (number of routes - 1), ensuring no negative values\n",
    "clean_df[\"num_transit_transfers\"] = clean_df[\"transit_routes_list\"].apply(lambda x: max(len(x.split(\", \")) - 1, 0) if x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8224cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Merge SP Survey fields:\n",
    "sp_other_airport_columns = [col for col in clean_df.columns if col.startswith(\"sp_other_airport_\")]\n",
    "\n",
    "\n",
    "# Create a new column with a comma-separated list of active modes\n",
    "clean_df[\"sp_other_airport_list\"] = clean_df[sp_other_airport_columns].apply(lambda row: \n",
    "    \", \".join([col.replace(\"sp_other_airport_\", \"\").replace(\"_\", \" \") for col in sp_other_airport_columns if row[col]==1]), \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b216f",
   "metadata": {},
   "source": [
    "### Add Passenger Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e6bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the `passenger_segment` column based on the updated logic\n",
    "clean_df[\"passenger_segment\"] = np.where(\n",
    "    # Resident Arriving\n",
    "    (clean_df[\"passenger_type\"] == e.PassengerType.ARRIVING) & \n",
    "    ((clean_df[\"resident_visitor_general\"] == e.ResidentVisitorGeneral.COMING_HOME) |\n",
    "     (clean_df[\"resident_visitor_followup\"] == e.ResidentVisitorFollowup.LIVE_OUTSIDE_REGION_TRAVELED_TO_AIRPORT)),\n",
    "    e.PassengerSegment.RESIDENT_ARRIVING,  # Resident Arriving\n",
    "    np.where(\n",
    "        (clean_df[\"passenger_type\"] == e.PassengerType.ARRIVING),\n",
    "        e.PassengerSegment.VISITOR_ARRIVING,  # Visitor Arriving\n",
    "        np.where(\n",
    "            (clean_df[\"passenger_type\"] == e.PassengerType.DEPARTING) & \n",
    "            ((clean_df[\"resident_visitor_general\"] == e.ResidentVisitorGeneral.LEAVING_HOME) |\n",
    "             (clean_df[\"resident_visitor_followup\"] == e.ResidentVisitorFollowup.LIVE_OUTSIDE_REGION_TRAVELED_TO_AIRPORT)),\n",
    "            e.PassengerSegment.RESIDENT_DEPARTING,  # Resident Departing\n",
    "            np.where(\n",
    "                # Visitor Departing\n",
    "                (clean_df[\"passenger_type\"] == e.PassengerType.DEPARTING),\n",
    "                e.PassengerSegment.VISITOR_DEPARTING,  # Visitor Departing\n",
    "                None  # Default case (if no conditions match)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10107bfb",
   "metadata": {},
   "source": [
    "### Add combined resident visitor status and flight purpose:\n",
    "\n",
    "Also adds EMPLOYEE as a segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b875c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[\"resident_visitor_purpose\"] = np.where(\n",
    "    # If the person is an employee\n",
    "    clean_df[\"marketsegment\"] == e.Type.EMPLOYEE,\n",
    "    e.ResidentVisitorPurpose.EMPLOYEE,\n",
    "\n",
    "    np.where(\n",
    "        # Resident Business\n",
    "        (clean_df[\"passenger_segment\"].isin([e.PassengerSegment.RESIDENT_ARRIVING, e.PassengerSegment.RESIDENT_DEPARTING])) & \n",
    "        (clean_df[\"flight_purpose\"].isin([e.FlightPurpose.BUSINESS_WORK, e.FlightPurpose.COMBINATION_BUSINESS_LEISURE])),\n",
    "        e.ResidentVisitorPurpose.RESIDENT_BUSINESS,\n",
    "\n",
    "        np.where(\n",
    "            # Resident Non-Business\n",
    "            clean_df[\"passenger_segment\"].isin([e.PassengerSegment.RESIDENT_ARRIVING, e.PassengerSegment.RESIDENT_DEPARTING]),\n",
    "            e.ResidentVisitorPurpose.RESIDENT_NON_BUSINESS,\n",
    "\n",
    "            np.where(\n",
    "                # Visitor Business\n",
    "                (clean_df[\"flight_purpose\"].isin([e.FlightPurpose.BUSINESS_WORK, e.FlightPurpose.COMBINATION_BUSINESS_LEISURE])),\n",
    "                e.ResidentVisitorPurpose.VISITOR_BUSINESS,\n",
    "\n",
    "                # Visitor Non-Business (default case)\n",
    "                e.ResidentVisitorPurpose.VISITOR_NON_BUSINESS\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70da9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['resident_visitor_purpose'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa9ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explicit Visitor Check\n",
    "clean_df[\"qualified_visitor\"] = np.where(\n",
    "    # Arriving and visiting or neither, and does not live in the same region traveled\n",
    "    (clean_df[\"passenger_type\"] == e.PassengerType.ARRIVING) & \n",
    "    ((clean_df[\"resident_visitor_general\"] == e.ResidentVisitorGeneral.VISITING) | \n",
    "     (clean_df[\"resident_visitor_general\"] == e.ResidentVisitorGeneral.NEITHER)) &\n",
    "    (clean_df[\"resident_visitor_followup\"] != e.ResidentVisitorFollowup.LIVE_OUTSIDE_REGION_TRAVELED_TO_AIRPORT),\n",
    "    1,  # Qualified visitor\n",
    "    np.where(\n",
    "        # Departing and going home or neither, and does not live in the same region traveled\n",
    "        (clean_df[\"passenger_type\"] == e.PassengerType.DEPARTING) &\n",
    "        ((clean_df[\"resident_visitor_general\"] == e.ResidentVisitorGeneral.GOING_HOME) | \n",
    "         (clean_df[\"resident_visitor_general\"] == e.ResidentVisitorGeneral.NEITHER)) &\n",
    "        (clean_df[\"resident_visitor_followup\"] != e.ResidentVisitorFollowup.LIVE_OUTSIDE_REGION_TRAVELED_TO_AIRPORT),\n",
    "        1,  # Qualified visitor\n",
    "        0  # Not a qualified visitor\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd18340",
   "metadata": {},
   "source": [
    "### Add some new consolidated variables, Combine some variables to exclude directionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8194a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## New changes:\n",
    "clean_df['number_of_nights'] = clean_df['nights_away'].fillna(clean_df['nights_visited'])\n",
    "\n",
    "# Set is_sdia_home_airport to 1 for resident arriving or departing passengers\n",
    "clean_df[\"is_sdia_home_airport\"] = np.where(\n",
    "    clean_df[\"passenger_segment\"].isin([e.PassengerSegment.RESIDENT_ARRIVING, e.PassengerSegment.RESIDENT_DEPARTING]), \n",
    "    1, \n",
    "    0\n",
    ")\n",
    "### Combining reverse_mode, as reverse_mode_combined - \n",
    "clean_df['reverse_mode_combined'] = clean_df['reverse_mode_grouped'].combine_first(clean_df['reverse_mode_predicted_grouped'])\n",
    "clean_df['reverse_mode_combined_other'] = clean_df['reverse_mode_predicted_other']\n",
    "\n",
    "clean_df['party_size_flight'] = clean_df['number_of_travel_companions'] + 1\n",
    "\n",
    "## party size\n",
    "clean_df[\"party_size_ground_access\"] = np.where(\n",
    "    clean_df[\"party_size_ground_access_same\"] == \"Yes\", \n",
    "    clean_df[\"party_size_flight\"], \n",
    "    clean_df[\"party_size_ground_access\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a90f77",
   "metadata": {},
   "source": [
    "Remove origin, destination coordinates, when the location does not make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c71af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For origin columns: if origin_state not in ['CA', 'BC'] OR origin_city is not 'Yuma'\n",
    "mask_origin = ((clean_df['origin_state'].isin(['CA', 'BC'])) | (clean_df['origin_city'] == 'Yuma'))\n",
    "clean_df.loc[~mask_origin, ['origin_latitude', 'origin_longitude']] = np.nan\n",
    "\n",
    "# For destination columns: if destination_state not in ['CA', 'BC'] OR destination_city is not 'Yuma'\n",
    "mask_destination = ((clean_df['destination_state'].isin(['CA', 'BC'])) | (clean_df['destination_city'] == 'Yuma'))\n",
    "clean_df.loc[~mask_destination, ['destination_latitude', 'destination_longitude']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d42d85",
   "metadata": {},
   "source": [
    "### Populate Home Location fields when it is not explicitly asked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab7aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks for home activity types\n",
    "mask_origin_home = clean_df['origin_activity_type'] == e.ActivityType.HOME\n",
    "mask_destination_home = clean_df['destination_activity_type'] == e.ActivityType.HOME\n",
    "\n",
    "# For rows where origin is home, copy the origin fields to home_location_ fields\n",
    "clean_df.loc[mask_origin_home, 'home_location_city'] = clean_df.loc[mask_origin_home, 'origin_city']\n",
    "clean_df.loc[mask_origin_home, 'home_location_state'] = clean_df.loc[mask_origin_home, 'origin_state']\n",
    "clean_df.loc[mask_origin_home, 'home_location_zip'] = clean_df.loc[mask_origin_home, 'origin_zip']\n",
    "clean_df.loc[mask_origin_home, 'home_location_latitude'] = clean_df.loc[mask_origin_home, 'origin_latitude']\n",
    "clean_df.loc[mask_origin_home, 'home_location_longitude'] = clean_df.loc[mask_origin_home, 'origin_longitude']\n",
    "clean_df.loc[mask_origin_home, 'home_location_municipal_zone'] = clean_df.loc[mask_origin_home, 'origin_municipal_zone']\n",
    "clean_df.loc[mask_origin_home, 'home_location_pmsa'] = clean_df.loc[mask_origin_home, 'origin_pmsa']\n",
    "\n",
    "# For rows where destination is home, copy the destination fields to home_location_ fields\n",
    "clean_df.loc[mask_destination_home, 'home_location_city'] = clean_df.loc[mask_destination_home, 'destination_city']\n",
    "clean_df.loc[mask_destination_home, 'home_location_state'] = clean_df.loc[mask_destination_home, 'destination_state']\n",
    "clean_df.loc[mask_destination_home, 'home_location_zip'] = clean_df.loc[mask_destination_home, 'destination_zip']\n",
    "clean_df.loc[mask_destination_home, 'home_location_latitude'] = clean_df.loc[mask_destination_home, 'destination_latitude']\n",
    "clean_df.loc[mask_destination_home, 'home_location_longitude'] = clean_df.loc[mask_destination_home, 'destination_longitude']\n",
    "clean_df.loc[mask_origin_home, 'home_location_municipal_zone'] = clean_df.loc[mask_origin_home, 'destination_municipal_zone']\n",
    "clean_df.loc[mask_origin_home, 'home_location_pmsa'] = clean_df.loc[mask_origin_home, 'destination_pmsa']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee60c2a1",
   "metadata": {},
   "source": [
    "#### Fix transit_boarding and alighting coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a45327",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['transit_boarding_latitude'] = (\n",
    "    clean_df['transit_board_1_lat']\n",
    "    .combine_first(clean_df['stop_on_latitude'])\n",
    ")\n",
    "\n",
    "clean_df['transit_boarding_longitude'] = (\n",
    "    clean_df['transit_board_1_long']\n",
    "    .combine_first(clean_df['stop_on_longitude'])\n",
    ")\n",
    "\n",
    "clean_df['transit_alighting_latitude'] = (\n",
    "    clean_df['transit_alight_4_lat']\n",
    "    .combine_first(clean_df['transit_alight_3_lat'])\n",
    "    .combine_first(clean_df['transit_alight_2_lat'])\n",
    "    .combine_first(clean_df['transit_alight_1_lat'])\n",
    "    .combine_first(clean_df['stop_off_latitude'])\n",
    ")\n",
    "\n",
    "clean_df['transit_alighting_longitude'] = (\n",
    "    clean_df['transit_alight_4_long']\n",
    "    .combine_first(clean_df['transit_alight_3_long'])\n",
    "    .combine_first(clean_df['transit_alight_2_long'])\n",
    "    .combine_first(clean_df['transit_alight_1_long'])\n",
    "    .combine_first(clean_df['stop_off_longitude'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5bddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv(clean_survey_file, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
